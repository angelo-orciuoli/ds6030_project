---
title: "DS6030 Project Data Evaluation"
author: "Tyler Hobbs"
date: "2025-06-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE)
knitr::opts_chunk$set(fig.align="center", fig.pos="tbh")
```

```{r}
#| message: FALSE
library(tidyverse)
library(GGally)
library(tidymodels)
library(discrim)
library(patchwork)
library(probably)
```


*Loading the data and Pre-processing*
```{r,warning=FALSE}
train<-read_csv("HaitiPixels.csv")
test1<-read_table("orthovnir057_ROI_NON_Blue_Tarps.txt",show_col_types = FALSE)
test1_sub<-test1[1:10]
colnames(test1_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test2<-read_table("orthovnir067_ROI_Blue_Tarps_data.txt",show_col_types = FALSE)
test3<-read_table("orthovnir067_ROI_Blue_Tarps.txt",show_col_types = FALSE)
test3_sub<-test3[1:10]
colnames(test3_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test4<-read_table("orthovnir067_ROI_NOT_Blue_Tarps.txt",show_col_types = FALSE)
test4_sub<-test4[1:10]
colnames(test4_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test5<-read_table("orthovnir069_ROI_Blue_Tarps.txt",show_col_types = FALSE)
test5_sub<-test5[1:10]
colnames(test5_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test6<-read_table("orthovnir069_ROI_NOT_Blue_Tarps.txt",show_col_types = FALSE)
test6_sub<-test6[1:10]
colnames(test6_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test7<-read_table("orthovnir078_ROI_Blue_Tarps.txt",show_col_types = FALSE)
test7_sub<-test7[1:10]
colnames(test7_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test8<-read_table("orthovnir078_ROI_NON_Blue_Tarps.txt",show_col_types = FALSE)
test8_sub<-test8[1:10]
colnames(test8_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")

combined_df_holdout<-bind_rows(test1_sub,test3_sub,test4_sub,test5_sub,test6_sub,test7_sub,test8_sub)
df_fig<-combined_df_holdout[8:10]
df_ID<-combined_df_holdout[1]
df_figID<-cbind(df_ID,df_fig)
lonlatcords<-combined_df_holdout[6:10]
trainwoclass<-train[2:4]
```

*Preliminary analysis and some Figures*
```{r}
df_long <- df_figID %>%
  pivot_longer(cols = c(B1, B2, B3),
               names_to = "Variable",
               values_to = "Value")
ggplot(data = df_long, aes(x = Variable, y = Value)) +
  geom_boxplot()
```
```{r}
ggplot(data = df_long, aes(x = Variable, y = Value)) +
  geom_bar(stat="identity",position="stack")
```
```{r}
ggplot(data = df_long, aes(x = Variable)) +
  geom_density()
```
*Proportions of Red, Green and Blue*
```{r}
long_df <- test2 %>% 
  pivot_longer(cols = c(B1, B2, B3), # Specify the columns to reshape
               names_to = "Category", # New column for original column names
               values_to = "Count")
ggplot(long_df,aes(x=Category,y=Count))+
  geom_bar(stat="identity",position="stack")
```
```{r}
ggplot(long_df,aes(x=Category,y=Count))+
  geom_boxplot()
```
```{r}
ggplot(long_df,aes(x=Category))+
  geom_density()
```
```{r}
long_df_2 <- train %>% 
  pivot_longer(cols = c(Red, Green, Blue), # Specify the columns to reshape
               names_to = "Category", # New column for original column names
               values_to = "Count")
ggplot(long_df_2,aes(x=Class,y=Count,fill=Category))+
  geom_bar(stat="identity",position="dodge")
ggplot(long_df_2,aes(x=Class,y=Category,fill=Count))+
  geom_bar(stat="identity",position="dodge")
```
```{r}
ggplot(long_df_2,aes(x=Class,y=Count))+
  geom_boxplot()
ggplot(long_df_2,aes(x=Category,y=Count))+
  geom_boxplot()
```
```{r}
ggplot(long_df_2,aes(x=Class,y=Count,fill=Category))+
  geom_point()
```
```{r}
#| fig.width: 10
#| fig.height: 5
ggplot(long_df_2,aes(x=Class,fill=Category))+
  geom_density(alpha=0.5)
ggplot(long_df_2,aes(x=Category,fill=Class))+
  geom_density(alpha=0.6)
```
```{r}
colors <- 'cornflowerblue'

limits <- list(continuous = wrap('points', color = colors),
              combo = wrap('box', fill = colors, outlier.color = colors))
ggpairs(df_fig,lower=limits,upper=limits,diag = list(continuous = wrap('densityDiag', fill = colors),
                    discrete = wrap('barDiag', fill = colors)))
```
```{r}
colors <- 'cornflowerblue'

limits <- list(continuous = wrap('points', color = colors),
              combo = wrap('box', fill = colors, outlier.color = colors))
ggpairs(trainwoclass,lower=limits,upper=limits,diag = list(continuous = wrap('densityDiag', fill = colors),
                    discrete = wrap('barDiag', fill = colors)))
```
B1:Red      B2:Green      B3: Blue

*Training* 
```{r}

Haiti_train<-train %>%
  mutate(type=case_when(
    Class == "Rooftop" ~ "Non-Blue_Tarp",
    Class == "Soil" ~ "Non-Blue_Tarp",
    Class == "Various Non-Tarp" ~ "Non-Blue_Tarp",
    Class == "Vegetation" ~ "Non-Blue_Tarp",
    Class == "Blue Tarp" ~ "Blue_Tarp"
  ))%>%
  mutate(Class=case_when(
    Class == "Rooftop" ~ "Non-Blue_Tarp",
    Class == "Soil" ~ "Non-Blue_Tarp",
    Class == "Various Non-Tarp" ~ "Non-Blue_Tarp",
    Class == "Vegetation" ~ "Non-Blue_Tarp",
    Class == "Blue Tarp" ~ "Blue_Tarp"
  ))%>%
    mutate(
        type=factor(type, levels=c("Blue_Tarp", "Non-Blue_Tarp")))%>% 
    mutate(
        group = paste(type, Class, sep="_"),
        group = factor(group),
    )
set.seed(1)
formula<-Class~Red + Green + Blue
Haiti_recipe <- recipe(formula, data=Haiti_train) %>%
    step_normalize(all_numeric_predictors())
```

```{r}
logreg_spec <- logistic_reg(mode="classification") %>%
    set_engine("glm")
lda_spec <- discrim_linear(mode="classification", engine="MASS")
qda_spec <- discrim_quad(mode="classification", engine="MASS")

logreg_wf <- workflow() %>%
    add_recipe(Haiti_recipe) %>%
    add_model(logreg_spec)
lda_wf <- workflow() %>%
    add_recipe(Haiti_recipe) %>%
    add_model(lda_spec)

qda_wf <- workflow() %>%
    add_recipe(Haiti_recipe) %>%
    add_model(qda_spec)
```

```{r}
resamples <- vfold_cv(Haiti_train, v=10, strata=type)
custom_metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=custom_metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=custom_metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=custom_metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(logreg_cv) %>%
        mutate(model="Logistic regression"),
    collect_metrics(lda_cv) %>%
        mutate(model="LDA"),
    collect_metrics(qda_cv) %>%
        mutate(model="QDA")
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Cross-validation performance metrics", digits=3)
```

```{r}
#| fig.cap: Cross-validation performance metrics
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```
```{r}
#| fig.width: 12
#| fig.height: 4
#| fig.cap: ROC curves based on cross-validation predictions
roc_cv_plot <- function(model_cv, model_name) {
    cv_predictions <- collect_predictions(model_cv)
    cv_roc <- cv_predictions %>%
        roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first")

    g <- autoplot(cv_roc) +
        labs(title=model_name)
    return(g)
}
g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g1 + g2 + g3
```

```{r}
#| fig.width: 10
#| fig.height: 10
bind_rows(
    collect_predictions(logreg_cv) %>% mutate(model="Logistic regression"),
    collect_predictions(lda_cv) %>% mutate(model="LDA"),
    collect_predictions(qda_cv) %>% mutate(model="QDA")
) %>%
    group_by(model) %>%
    roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first") %>%
    autoplot()+labs(title="Combined ROC plot")
```
```{r}
tuningtrain<-Haiti_train[1:4]
```


**Random Forests:**

```{r}
rec_select <- recipe(Class ~ ., data=tuningtrain) %>%
    step_normalize(all_numeric_predictors()) %>%
    step_select_forests(all_predictors(), outcome="Class", top_p = tune())
forest<-rand_forest(mode="classification", trees=100, mtry=3) %>%
    set_engine("ranger")
forests_workflow <- workflow() %>%
    add_recipe(rec_select) %>%
    add_model(forest)
parameters <- extract_parameter_set_dials(neighbors_workflow)
parameters %>% knitr::kable()
```
```{r}
set.seed(1)
tune_results_forest <- tune_grid(forests_workflow,
                          resamples=vfold_cv(tuningtrain),
                          grid=grid_regular(parameters))
```
```{r}
autoplot(tune_results_forest)
```
```{r}
show_best(tune_results_forest,metric="roc_auc")
```
```{r}
#| warning: false 
#| message: false
best_parameters <- select_best(tune_results_forest, metric="roc_auc")
best_workflow <- forests_workflow %>%
    finalize_workflow(best_parameters) %>%
    fit(tuningtrain)
best_workflow
```
```{r}
#| warning: false 
#| message: false
resamples <- vfold_cv(tuningtrain, v=10,strata=Class)
custom_metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)

forest_cv <- fit_resamples(best_workflow, resamples, metrics=custom_metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(forest_cv) %>%
        mutate(model="Random Forest"),
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Random Forest Metrics", digits=3)
```
```{r}
#| fig.cap: Random Forest performance metrics 
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%

cv_metrics <- bind_rows(
    collect_metrics(forest_cv) %>% mutate(model="Random Forest"),
)
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```
```{r}
#| fig.width: 10
#| fig.height: 10
bind_rows(
    collect_predictions(forest_cv) %>% mutate(model="Random Forest"),
) %>%
    group_by(model) %>%
    roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first") %>%
    autoplot()+labs(title="Random Forest ROC plot")
```
**Penalized Logistic Regression (elastic net penalty):**

```{r}
formula<-Class~Red+Green+Blue

recipe_spec <- recipe(formula, data=tuningtrain) %>%
    step_dummy(all_nominal(), -all_outcomes())
```

```{r}
model_glmnet <- logistic_reg(engine="glmnet", mode="classification",
                           penalty=tune(), mixture=tune())
wf <- workflow() %>%
    add_model(model_glmnet) %>%
    add_recipe(recipe_spec)
```

```{r}
parameters <- extract_parameter_set_dials(wf) %>%
    update(penalty=penalty(c(-4, -1)))
tune_wf <- tune_bayes(wf, resamples=resamples, metrics=custom_metrics,
                      param_info=parameters, iter=25)
```

```{r}
autoplot(tune_wf)
```

```{r}
best_parameter <- select_best(tune_wf, metric="roc_auc")
best_wf <- finalize_workflow(wf, best_parameter)
```

```{r}
result_cv <-  fit_resamples(best_wf, resamples,
                            metrics=custom_metrics, control=cv_control)
fitted_model <- best_wf %>% fit(tuningtrain)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(result_cv) %>%
        mutate(model="Penalized Logistic Regression"),
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Penalized Logistic Regression Metrics", digits=3)
```

```{r}
#| fig.cap: Random Forest performance metrics 
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%

cv_metrics <- bind_rows(
    collect_metrics(result_cv) %>% mutate(model="Penalized Logistic Regression"),
)
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```
```{r}
#| fig.width: 10
#| fig.height: 10
bind_rows(
    collect_predictions(result_cv) %>% mutate(model="Penalized Logistic Regression"),
) %>%
    group_by(model) %>%
    roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first") %>%
    autoplot()+labs(title="Penalized Logistic Regression ROC plot")
```

