---
title: "DS6030 Project Data Evaluation"
author: "Tyler Hobbs"
date: "2025-06-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE)
knitr::opts_chunk$set(fig.align="center", fig.pos="tbh")
```

```{r}
#| message: FALSE
library(tidyverse)
library(GGally)
library(tidymodels)
library(discrim)
library(patchwork)
library(probably)
library(vip)
```

*Loading the data and Pre-processing*
```{r,warning=FALSE}
train<-read_csv("HaitiPixels.csv")
test1<-read_table("orthovnir057_ROI_NON_Blue_Tarps.txt",show_col_types = FALSE)
test1_sub<-test1[1:10]
colnames(test1_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test2<-read_table("orthovnir067_ROI_Blue_Tarps_data.txt",show_col_types = FALSE)
test3<-read_table("orthovnir067_ROI_Blue_Tarps.txt",show_col_types = FALSE)
test3_sub<-test3[1:10]
colnames(test3_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test4<-read_table("orthovnir067_ROI_NOT_Blue_Tarps.txt",show_col_types = FALSE)
test4_sub<-test4[1:10]
colnames(test4_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test5<-read_table("orthovnir069_ROI_Blue_Tarps.txt",show_col_types = FALSE)
test5_sub<-test5[1:10]
colnames(test5_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test6<-read_table("orthovnir069_ROI_NOT_Blue_Tarps.txt",show_col_types = FALSE)
test6_sub<-test6[1:10]
colnames(test6_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test7<-read_table("orthovnir078_ROI_Blue_Tarps.txt",show_col_types = FALSE)
test7_sub<-test7[1:10]
colnames(test7_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")
test8<-read_table("orthovnir078_ROI_NON_Blue_Tarps.txt",show_col_types = FALSE)
test8_sub<-test8[1:10]
colnames(test8_sub)<-c("ID","X","Y","Map X","Map Y", "Lat","Lon","B1","B2","B3")

combined_df_holdout<-bind_rows(test1_sub,test3_sub,test4_sub,test5_sub,test6_sub,test7_sub,test8_sub)
df_fig<-combined_df_holdout[8:10]
df_ID<-combined_df_holdout[1]
df_figID<-cbind(df_ID,df_fig)
lonlatcords<-combined_df_holdout[6:10]
trainwoclass<-train[2:4]
```
**Pre-processing**
```{r}
NonBlueholdout<-bind_rows(test1_sub,test4_sub,test6_sub,test7_sub,test8_sub)
NonBlueholdout_new<-NonBlueholdout[8:10]
names(NonBlueholdout_new)[1]<-"Red"
names(NonBlueholdout_new)[2]<-"Green"
names(NonBlueholdout_new)[3]<-"Blue"
NonBlueholdout_new <- NonBlueholdout_new %>%
  mutate(Class=1)
NonBlueholdout_new <- NonBlueholdout_new %>%
mutate(Class=factor(Class,labels=c("1")))%>% 
  mutate(Class=case_when(
    Class == 1 ~ "Non-Blue_Tarp"
  ))
Blueholdout<-bind_rows(test2,test3_sub,test5_sub)
Blueholdout_new<-Blueholdout[1:3]
names(Blueholdout_new)[1]<-"Red"
names(Blueholdout_new)[2]<-"Green"
names(Blueholdout_new)[3]<-"Blue"
Blueholdout_new <- Blueholdout_new %>%
  mutate(Class=1)
Blueholdout_new <- Blueholdout_new %>%
mutate(Class=factor(Class,labels=c("1")))%>% 
  mutate(Class=case_when(
    Class == 1 ~ "Blue_Tarp"
  ))
holdout<-bind_rows(Blueholdout_new,NonBlueholdout_new)
```

```{r}
holdout<-holdout %>%
  mutate(type=case_when(
    Class == "Non-Blue_Tarp" ~ 0,
    Class == "Blue_Tarp" ~ 1
    ))
```

```{r}
holdout<-holdout %>%
    mutate(
        group = paste(type, Class, sep="_"),
        group = factor(group),
    )
```


*Training* 
```{r}

Haiti_train<-train %>%
  mutate(type=case_when(
    Class == "Rooftop" ~ "Non-Blue_Tarp",
    Class == "Soil" ~ "Non-Blue_Tarp",
    Class == "Various Non-Tarp" ~ "Non-Blue_Tarp",
    Class == "Vegetation" ~ "Non-Blue_Tarp",
    Class == "Blue Tarp" ~ "Blue_Tarp"
  ))%>%
  mutate(Class=case_when(
    Class == "Rooftop" ~ "Non-Blue_Tarp",
    Class == "Soil" ~ "Non-Blue_Tarp",
    Class == "Various Non-Tarp" ~ "Non-Blue_Tarp",
    Class == "Vegetation" ~ "Non-Blue_Tarp",
    Class == "Blue Tarp" ~ "Blue_Tarp"
  ))%>%
    mutate(
        type=factor(type, levels=c("Blue_Tarp", "Non-Blue_Tarp")))%>% 
    mutate(
        group = paste(type, Class, sep="_"),
        group = factor(group),
    )
set.seed(1)
formula<-Class~Red + Green + Blue
Haiti_recipe <- recipe(formula, data=Haiti_train) %>%
    step_normalize(all_numeric_predictors())
names(df_fig)[1]<-"Red"
names(df_fig)[2]<-"Green"
names(df_fig)[3]<-"Blue"
```

```{r}
logreg_spec <- logistic_reg(mode="classification") %>%
    set_engine("glm")
lda_spec <- discrim_linear(mode="classification", engine="MASS")
qda_spec <- discrim_quad(mode="classification", engine="MASS")

logreg_wf <- workflow() %>%
    add_recipe(Haiti_recipe) %>%
    add_model(logreg_spec)
lda_wf <- workflow() %>%
    add_recipe(Haiti_recipe) %>%
    add_model(lda_spec)

qda_wf <- workflow() %>%
    add_recipe(Haiti_recipe) %>%
    add_model(qda_spec)
```

```{r}
resamples <- vfold_cv(Haiti_train, v=10, strata=type)
custom_metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=custom_metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=custom_metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=custom_metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(logreg_cv) %>%
        mutate(model="Logistic regression"),
    collect_metrics(lda_cv) %>%
        mutate(model="LDA"),
    collect_metrics(qda_cv) %>%
        mutate(model="QDA")
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Cross-validation performance metrics", digits=5)
```

```{r}
#| fig.cap: Cross-validation performance metrics
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```
```{r}
#| fig.width: 12
#| fig.height: 4
#| fig.cap: ROC curves based on cross-validation predictions
roc_cv_plot <- function(model_cv, model_name) {
    cv_predictions <- collect_predictions(model_cv)
    cv_roc <- cv_predictions %>%
        roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first")

    g <- autoplot(cv_roc) +
        labs(title=model_name)
    return(g)
}
g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g1 + g2 + g3
```

```{r}
#| fig.width: 10
#| fig.height: 10
bind_rows(
    collect_predictions(logreg_cv) %>% mutate(model="Logistic regression"),
    collect_predictions(lda_cv) %>% mutate(model="LDA"),
    collect_predictions(qda_cv) %>% mutate(model="QDA")
) %>%
    group_by(model) %>%
    roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first") %>%
    autoplot()+labs(title="Combined ROC plot")
```
```{r}
resamples <- vfold_cv(holdout, v=10, strata=type)
custom_metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=custom_metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=custom_metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=custom_metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(logreg_cv) %>%
        mutate(model="Logistic regression"),
    collect_metrics(lda_cv) %>%
        mutate(model="LDA"),
    collect_metrics(qda_cv) %>%
        mutate(model="QDA")
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Cross-validation performance metrics", digits=5)
```

```{r}
Haiti_train<-train %>%
  mutate(type=case_when(
    Class == "Rooftop" ~ "Non-Blue_Tarp",
    Class == "Soil" ~ "Non-Blue_Tarp",
    Class == "Various Non-Tarp" ~ "Non-Blue_Tarp",
    Class == "Vegetation" ~ "Non-Blue_Tarp",
    Class == "Blue Tarp" ~ "Blue_Tarp"
  ))%>%
  mutate(Class=case_when(
    Class == "Rooftop" ~ "Non-Blue_Tarp",
    Class == "Soil" ~ "Non-Blue_Tarp",
    Class == "Various Non-Tarp" ~ "Non-Blue_Tarp",
    Class == "Vegetation" ~ "Non-Blue_Tarp",
    Class == "Blue Tarp" ~ "Blue_Tarp"
  ))%>%
    mutate(
        type=factor(type, levels=c("Blue_Tarp", "Non-Blue_Tarp")))%>% 
    mutate(
        group = paste(type, Class, sep="_"),
        group = factor(group),
    )
set.seed(1)
formula<-Class~Red + Green + Blue
Haiti_recipe <- recipe(formula, data=Haiti_train) %>%
    step_normalize(all_numeric_predictors())
tuningtrain<-Haiti_train[1:4]
tuningtrain_sub<- tuningtrain %>%
  mutate(Class=factor(Class,labels=c("Non-Blue_Tarp","Blue_Tarp")))%>% 
  mutate(case_when(
    Class == "Blue_Tarp" ~ 1,
    Class == "Non-Blue_Tarp" ~ 0 
  ))
```


**Random Forests:**

```{r}
tuningtrain$Red<-as.numeric(tuningtrain$Red)
tuningtrain$Green<-as.numeric(tuningtrain$Green)
tuningtrain$Blue<-as.numeric(tuningtrain$Blue)
tuningtrain$Class<-as.character(tuningtrain$Class)
formula<-Class~ Red + Green + Blue
rec <- recipe(formula, data=tuningtrain)
class(tuningtrain$Class)
```


```{r}
forest<-rand_forest(mode="classification", trees=500,min_n = tune(), mtry=tune()) %>%
    set_engine("ranger",importance="impurity")

forests_workflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(forest)

```

```{r}
parameters <- extract_parameter_set_dials(forests_workflow)%>%
update(
mtry = mtry(c(1, 3)),
min_n = min_n(c(2, 40))
)
```

```{r}
set.seed(1)
tune_results_forest <- tune_grid(forests_workflow,
                          resamples=vfold_cv(tuningtrain),
                          grid=grid_regular(parameters))
```

```{r}
autoplot(tune_results_forest)
```
```{r}
show_best(tune_results_forest,metric="roc_auc")
```
```{r}
#| warning: false 
#| message: false
best_parameters <- select_best(tune_results_forest, metric="roc_auc")
best_workflow <- forests_workflow %>%
    finalize_workflow(best_parameters) %>%
    fit(tuningtrain)
best_workflow
```
```{r}
#| warning: false 
#| message: false
resamples <- vfold_cv(tuningtrain, v=10,strata=Class)
custom_metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)

forest_cv <- fit_resamples(best_workflow, resamples, metrics=custom_metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(forest_cv) %>%
        mutate(model="Random Forest"),
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Random Forest Metrics", digits=5)
```
```{r}
#| fig.cap: Random Forest performance metrics 
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%

cv_metrics <- bind_rows(
    collect_metrics(forest_cv) %>% mutate(model="Random Forest"),
)
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```
```{r}
#| fig.width: 10
#| fig.height: 10
bind_rows(
    collect_predictions(forest_cv) %>% mutate(model="Random Forest"),
) %>%
    group_by(model) %>%
    roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first") %>%
    autoplot()+labs(title="Random Forest ROC plot")
```
```{r}
best_workflow %>% 
    extract_fit_engine() %>%
    vip()
```

```{r}
result_cvh <-  fit_resamples(best_workflow, resamples,
                            metrics=custom_metrics, control=cv_control)
fitted_model <- best_workflow %>% fit(holdout)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(result_cvh) %>%
        mutate(model="Random Forests"),
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Random Forests Metrics", digits=5)
```



**Penalized Logistic Regression (elastic net penalty):**

```{r}
formula<-Class~Red+Green+Blue

recipe_spec <- recipe(formula, data=tuningtrain) %>%
    step_dummy(all_nominal(), -all_outcomes())
```

```{r}
model_glmnet <- logistic_reg(engine="glmnet", mode="classification",
                           penalty=tune(), mixture=0.5)
wf <- workflow() %>%
    add_model(model_glmnet) %>%
    add_recipe(recipe_spec)
```

```{r}
parameters <- extract_parameter_set_dials(wf) %>%
    update(penalty=penalty(c(-4, -1)))
tune_wf <- tune_bayes(wf, resamples=resamples, metrics=custom_metrics,
                      param_info=parameters, iter=25)
```

```{r}
autoplot(tune_wf)
```

```{r}
best_parameter <- select_best(tune_wf, metric="roc_auc")
best_wf <- finalize_workflow(wf, best_parameter)
```

```{r}
result_cv <-  fit_resamples(best_wf, resamples,
                            metrics=custom_metrics, control=cv_control)
fitted_model <- best_wf %>% fit(tuningtrain)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(result_cv) %>%
        mutate(model="Penalized Logistic Regression"),
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Penalized Logistic Regression Metrics", digits=5)
```

```{r}
#| fig.cap: Penalized Logistic Regression performance metrics 
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%

cv_metrics <- bind_rows(
    collect_metrics(result_cv) %>% mutate(model="Penalized Logistic Regression"),
)
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```
```{r}
#| fig.width: 10
#| fig.height: 10
bind_rows(
    collect_predictions(result_cv) %>% mutate(model="Penalized Logistic Regression"),
) %>%
    group_by(model) %>%
    roc_curve(truth=Class, .pred_Blue_Tarp, event_level="first") %>%
    autoplot()+labs(title="Penalized Logistic Regression ROC plot")
```
```{r}
result_cvh <-  fit_resamples(best_wf, resamples,
                            metrics=custom_metrics, control=cv_control)
fitted_model <- best_wf %>% fit(holdout)
```

```{r}
cv_metrics <- bind_rows(
    collect_metrics(result_cvh) %>%
        mutate(model="Penalized Logistic Regression Holdout"),
)
cv_metrics %>%
    dplyr::select(model, .metric, mean) %>%
    pivot_wider(names_from=.metric, values_from=mean) %>%
    knitr::kable(caption="Penalized Logistic Regression Holdout Metrics", digits=5)
```
